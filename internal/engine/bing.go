package engine

import (
	"context"
	"fmt"
	"io"
	"log"
	"net/http"
	"net/http/cookiejar"
	"net/url"
	"regexp"
	"strings"
	"time"

	"github.com/PuerkitoBio/goquery"
)

// BingEngine Bing æœç´¢å¼•æ“å®ç°
type BingEngine struct {
	client   *http.Client
	proxyURL string
}

// NewBingEngine åˆ›å»º Bing æœç´¢å¼•æ“å®ä¾‹
func NewBingEngine(proxyURL string) *BingEngine {
	jar, _ := cookiejar.New(nil)
	
	transport := &http.Transport{}
	if proxyURL != "" {
		if proxy, err := url.Parse(proxyURL); err == nil {
			transport.Proxy = http.ProxyURL(proxy)
		}
	}

	client := &http.Client{
		Timeout:   30 * time.Second,
		Jar:       jar,
		Transport: transport,
	}

	return &BingEngine{
		client:   client,
		proxyURL: proxyURL,
	}
}

// Name è¿”å›å¼•æ“åç§°
func (e *BingEngine) Name() string {
	return "bing"
}

// Search æ‰§è¡Œ Bing æœç´¢
func (e *BingEngine) Search(ctx context.Context, query string, limit int) ([]SearchResult, error) {
	var allResults []SearchResult
	pn := 0

	for len(allResults) < limit {
		results, err := e.searchPage(ctx, query, pn)
		if err != nil {
			if len(allResults) > 0 {
				// å¦‚æœå·²ç»æœ‰ä¸€äº›ç»“æœï¼Œå°±è¿”å›è¿™äº›
				break
			}
			return nil, err
		}

		if len(results) == 0 {
			break
		}

		allResults = append(allResults, results...)
		pn++

		if pn > 5 {
			break
		}
	}

	if len(allResults) > limit {
		allResults = allResults[:limit]
	}

	return allResults, nil
}

// searchPage æœç´¢å•é¡µç»“æœ
func (e *BingEngine) searchPage(ctx context.Context, query string, page int) ([]SearchResult, error) {
	// æ„å»ºè¯·æ±‚ URL - ä½¿ç”¨å›½é™…ç‰ˆ
	searchURL := fmt.Sprintf("https://www.bing.com/search?q=%s&first=%d&setlang=en",
		url.QueryEscape(query), 1+page*10)

	req, err := http.NewRequestWithContext(ctx, "GET", searchURL, nil)
	if err != nil {
		return nil, fmt.Errorf("create request failed: %w", err)
	}

	e.setHeaders(req)

	resp, err := e.client.Do(req)
	if err != nil {
		return nil, fmt.Errorf("request failed: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		body, _ := io.ReadAll(resp.Body)
		return nil, fmt.Errorf("unexpected status code: %d, body: %s", resp.StatusCode, string(body[:min(len(body), 200)]))
	}

	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, fmt.Errorf("read body failed: %w", err)
	}

	bodyStr := string(body)
	log.Printf("ğŸ” Bing response size: %d bytes", len(body))

	// é¦–å…ˆå°è¯•æ ‡å‡†è§£æ
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(bodyStr))
	if err != nil {
		return nil, fmt.Errorf("parse HTML failed: %w", err)
	}

	results := e.parseResults(doc)
	
	// å¦‚æœæ ‡å‡†è§£ææ²¡æœ‰ç»“æœï¼Œå°è¯•æ­£åˆ™åŒ¹é…
	if len(results) == 0 {
		log.Printf("âš ï¸ Standard parsing found no results, trying regex extraction")
		results = e.extractResultsWithRegex(bodyStr)
	}

	log.Printf("ğŸ” Bing page %d: found %d results", page, len(results))
	return results, nil
}

// setHeaders è®¾ç½®è¯·æ±‚å¤´
func (e *BingEngine) setHeaders(req *http.Request) {
	req.Header.Set("User-Agent", "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
	req.Header.Set("Accept", "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8")
	req.Header.Set("Accept-Language", "en-US,en;q=0.9")
	req.Header.Set("Cache-Control", "no-cache")
	req.Header.Set("Pragma", "no-cache")
	req.Header.Set("Sec-Fetch-Dest", "document")
	req.Header.Set("Sec-Fetch-Mode", "navigate")
	req.Header.Set("Sec-Fetch-Site", "none")
	req.Header.Set("Sec-Fetch-User", "?1")
	req.Header.Set("Upgrade-Insecure-Requests", "1")
}

// parseResults è§£ææœç´¢ç»“æœ
func (e *BingEngine) parseResults(doc *goquery.Document) []SearchResult {
	var results []SearchResult

	// å°è¯•å¤šç§é€‰æ‹©å™¨
	selectors := []string{
		"li.b_algo",
		"#b_results > li.b_algo",
		".b_algo",
	}

	for _, selector := range selectors {
		doc.Find(selector).Each(func(i int, s *goquery.Selection) {
			titleEl := s.Find("h2")
			linkEl := s.Find("h2 a")

			if titleEl.Length() == 0 || linkEl.Length() == 0 {
				return
			}

			href, exists := linkEl.Attr("href")
			if !exists || !strings.HasPrefix(href, "http") {
				return
			}

			description := ""
			descSelectors := []string{".b_caption p", "p", ".b_algoSlug"}
			for _, descSel := range descSelectors {
				descEl := s.Find(descSel)
				if descEl.Length() > 0 {
					description = strings.TrimSpace(descEl.First().Text())
					if description != "" {
						break
					}
				}
			}

			source := ""
			citeEl := s.Find("cite")
			if citeEl.Length() > 0 {
				source = strings.TrimSpace(citeEl.First().Text())
			}

			results = append(results, SearchResult{
				Title:       strings.TrimSpace(titleEl.Text()),
				URL:         href,
				Description: description,
				Source:      source,
				Engine:      "bing",
			})
		})

		if len(results) > 0 {
			break
		}
	}

	return results
}

// extractResultsWithRegex ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ç»“æœï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
func (e *BingEngine) extractResultsWithRegex(html string) []SearchResult {
	var results []SearchResult

	// å°è¯•åŒ¹é… Bing ç»“æœçš„ URL å’Œæ ‡é¢˜æ¨¡å¼
	// è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„æ­£åˆ™ï¼Œå¯èƒ½éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
	urlPattern := regexp.MustCompile(`<a[^>]*href="(https?://[^"]+)"[^>]*>([^<]+)</a>`)
	matches := urlPattern.FindAllStringSubmatch(html, -1)

	seen := make(map[string]bool)
	for _, match := range matches {
		if len(match) < 3 {
			continue
		}
		
		href := match[1]
		title := strings.TrimSpace(match[2])

		// è¿‡æ»¤æ‰ Bing è‡ªèº«çš„é“¾æ¥å’Œç©ºæ ‡é¢˜
		if strings.Contains(href, "bing.com") || 
		   strings.Contains(href, "microsoft.com") ||
		   title == "" ||
		   seen[href] {
			continue
		}

		// ç®€å•è¿‡æ»¤ï¼Œåªä¿ç•™çœ‹èµ·æ¥åƒæœç´¢ç»“æœçš„é“¾æ¥
		if !strings.HasPrefix(href, "http") {
			continue
		}

		seen[href] = true
		results = append(results, SearchResult{
			Title:       title,
			URL:         href,
			Description: "",
			Source:      "",
			Engine:      "bing",
		})

		if len(results) >= 10 {
			break
		}
	}

	return results
}

func min(a, b int) int {
	if a < b {
		return a
	}
	return b
}
